{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./input/covid19countryinfo.csv\n",
      "./input/SIR_data.csv\n",
      "./input/states-daily.csv\n",
      "./input/covid19-deepscore.csv\n",
      "./input/population_data.csv\n",
      "./input/full-list-total-tests-for-covid-19.csv\n",
      "./input/country_codes.csv\n",
      "./input/enriched_covid_19_week_2.csv\n",
      "./input/covid19-global-forecasting-week-3/test.csv\n",
      "./input/covid19-global-forecasting-week-3/submission.csv\n",
      "./input/covid19-global-forecasting-week-3/train.csv\n",
      "./input/korea/SeoulFloating.csv\n",
      "./input/korea/TimeAge.csv\n",
      "./input/korea/SearchTrend.csv\n",
      "./input/korea/TimeProvince.csv\n",
      "./input/korea/Weather.csv\n",
      "./input/korea/PatientRoute.csv\n",
      "./input/korea/PatientInfo.csv\n",
      "./input/korea/Region.csv\n",
      "./input/korea/TimeGender.csv\n",
      "./input/korea/Case.csv\n",
      "./input/korea/Time.csv\n",
      "./input/covid19-global-forecasting-week-2/test.csv\n",
      "./input/covid19-global-forecasting-week-2/submission.csv\n",
      "./input/covid19-global-forecasting-week-2/train.csv\n",
      "./input/covidAPI/ESP.json\n",
      "./input/covidAPI/ICL.json\n",
      "./input/covidAPI/CHN.json\n",
      "./input/covidAPI/FRA.json\n",
      "./input/covidAPI/THA.json\n",
      "./input/covidAPI/DNK.json\n",
      "./input/covidAPI/DEU.json\n",
      "./input/covidAPI/KOR.json\n",
      "./input/covidAPI/TWN.json\n",
      "./input/covidAPI/ISL.json\n",
      "./input/covidAPI/ITA.json\n",
      "./input/.ipynb_checkpoints/SIR_data-checkpoint.csv\n",
      "./input/.ipynb_checkpoints/covid19-deepscore-checkpoint.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import json\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "\n",
    "for dirname, _, filenames in os.walk('./input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Code</th>\n",
       "      <th>Date</th>\n",
       "      <th>confirmed</th>\n",
       "      <th>confirmed_noise</th>\n",
       "      <th>infectious_noise_rate</th>\n",
       "      <th>infectious_rate</th>\n",
       "      <th>removed</th>\n",
       "      <th>removed_noise</th>\n",
       "      <th>removed_noise_rate</th>\n",
       "      <th>removed_rate</th>\n",
       "      <th>susceptible</th>\n",
       "      <th>susceptible_noise</th>\n",
       "      <th>susceptible_noise_rate</th>\n",
       "      <th>susceptible_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>155</td>\n",
       "      <td>SIR-large-249</td>\n",
       "      <td>2020-07-05</td>\n",
       "      <td>6.813406e-09</td>\n",
       "      <td>8.345405e-09</td>\n",
       "      <td>4.172703e-14</td>\n",
       "      <td>3.406703e-14</td>\n",
       "      <td>1744.099053</td>\n",
       "      <td>1744.099053</td>\n",
       "      <td>0.00872</td>\n",
       "      <td>0.00872</td>\n",
       "      <td>198255.900947</td>\n",
       "      <td>198255.900947</td>\n",
       "      <td>0.99128</td>\n",
       "      <td>0.99128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>156</td>\n",
       "      <td>SIR-large-249</td>\n",
       "      <td>2020-07-06</td>\n",
       "      <td>3.576303e-09</td>\n",
       "      <td>3.202185e-09</td>\n",
       "      <td>1.601093e-14</td>\n",
       "      <td>1.788151e-14</td>\n",
       "      <td>1744.099053</td>\n",
       "      <td>1744.099053</td>\n",
       "      <td>0.00872</td>\n",
       "      <td>0.00872</td>\n",
       "      <td>198255.900947</td>\n",
       "      <td>198255.900947</td>\n",
       "      <td>0.99128</td>\n",
       "      <td>0.99128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>157</td>\n",
       "      <td>SIR-large-249</td>\n",
       "      <td>2020-07-07</td>\n",
       "      <td>1.854928e-09</td>\n",
       "      <td>1.278237e-09</td>\n",
       "      <td>6.391185e-15</td>\n",
       "      <td>9.274641e-15</td>\n",
       "      <td>1744.099053</td>\n",
       "      <td>1744.099053</td>\n",
       "      <td>0.00872</td>\n",
       "      <td>0.00872</td>\n",
       "      <td>198255.900947</td>\n",
       "      <td>198255.900947</td>\n",
       "      <td>0.99128</td>\n",
       "      <td>0.99128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>158</td>\n",
       "      <td>SIR-large-249</td>\n",
       "      <td>2020-07-08</td>\n",
       "      <td>9.504876e-10</td>\n",
       "      <td>1.032974e-09</td>\n",
       "      <td>5.164868e-15</td>\n",
       "      <td>4.752438e-15</td>\n",
       "      <td>1744.099053</td>\n",
       "      <td>1744.099053</td>\n",
       "      <td>0.00872</td>\n",
       "      <td>0.00872</td>\n",
       "      <td>198255.900947</td>\n",
       "      <td>198255.900947</td>\n",
       "      <td>0.99128</td>\n",
       "      <td>0.99128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>159</td>\n",
       "      <td>SIR-large-249</td>\n",
       "      <td>2020-07-09</td>\n",
       "      <td>4.810533e-10</td>\n",
       "      <td>6.917720e-10</td>\n",
       "      <td>3.458860e-15</td>\n",
       "      <td>2.405266e-15</td>\n",
       "      <td>1744.099053</td>\n",
       "      <td>1744.099053</td>\n",
       "      <td>0.00872</td>\n",
       "      <td>0.00872</td>\n",
       "      <td>198255.900947</td>\n",
       "      <td>198255.900947</td>\n",
       "      <td>0.99128</td>\n",
       "      <td>0.99128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0           Code        Date     confirmed  confirmed_noise  \\\n",
       "119995         155  SIR-large-249  2020-07-05  6.813406e-09     8.345405e-09   \n",
       "119996         156  SIR-large-249  2020-07-06  3.576303e-09     3.202185e-09   \n",
       "119997         157  SIR-large-249  2020-07-07  1.854928e-09     1.278237e-09   \n",
       "119998         158  SIR-large-249  2020-07-08  9.504876e-10     1.032974e-09   \n",
       "119999         159  SIR-large-249  2020-07-09  4.810533e-10     6.917720e-10   \n",
       "\n",
       "        infectious_noise_rate  infectious_rate      removed  removed_noise  \\\n",
       "119995           4.172703e-14     3.406703e-14  1744.099053    1744.099053   \n",
       "119996           1.601093e-14     1.788151e-14  1744.099053    1744.099053   \n",
       "119997           6.391185e-15     9.274641e-15  1744.099053    1744.099053   \n",
       "119998           5.164868e-15     4.752438e-15  1744.099053    1744.099053   \n",
       "119999           3.458860e-15     2.405266e-15  1744.099053    1744.099053   \n",
       "\n",
       "        removed_noise_rate  removed_rate    susceptible  susceptible_noise  \\\n",
       "119995             0.00872       0.00872  198255.900947      198255.900947   \n",
       "119996             0.00872       0.00872  198255.900947      198255.900947   \n",
       "119997             0.00872       0.00872  198255.900947      198255.900947   \n",
       "119998             0.00872       0.00872  198255.900947      198255.900947   \n",
       "119999             0.00872       0.00872  198255.900947      198255.900947   \n",
       "\n",
       "        susceptible_noise_rate  susceptible_rate  \n",
       "119995                 0.99128           0.99128  \n",
       "119996                 0.99128           0.99128  \n",
       "119997                 0.99128           0.99128  \n",
       "119998                 0.99128           0.99128  \n",
       "119999                 0.99128           0.99128  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"./input/SIR_data.csv\")\n",
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_in_sequence = 21\n",
    "output_days = 7\n",
    "\n",
    "sequence_length = days_in_sequence - 1\n",
    "training_percentage = 0.9\n",
    "temp_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 20, 1)             0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 20, 64)            16896     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 119       \n",
      "_________________________________________________________________\n",
      "removed (LeakyReLU)          (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 29,959\n",
      "Trainable params: 29,959\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#temporal input branch\n",
    "temporal_input_layer = Input(shape=(sequence_length,temp_dim))\n",
    "main_rnn_layer = layers.LSTM(64, return_sequences=True, recurrent_dropout=0.2)(temporal_input_layer)\n",
    "\n",
    "# removed output branch\n",
    "rnn_f = layers.LSTM(32)(main_rnn_layer)\n",
    "dense_f = layers.Dense(16)(rnn_f)\n",
    "dropout_f = layers.Dropout(0.2)(dense_f)\n",
    "output_f = layers.Dense(7)(dropout_f)\n",
    "removed = layers.LeakyReLU(alpha=0.1, name=\"removed\")(output_f)\n",
    "\n",
    "model = Model([temporal_input_layer], [removed])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [ReduceLROnPlateau(monitor='val_loss', verbose=1, factor=0.6), #patience=4  #EarlyStopping(monitor='val_loss', patience=20)\n",
    "             ModelCheckpoint(filepath='best_removed_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "model.compile(loss=[tf.keras.losses.mean_squared_logarithmic_error], optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_list = []\n",
    "\n",
    "for i in range(250):\n",
    "    code = \"SIR-small-{}\".format(i)\n",
    "    country_df = train_df.query(f\"Code=='{code}'\")\n",
    "    # Added a quick hack to double the number of sequences\n",
    "    # Warning: This will later create a minor leakage from the training set into the validation set.\n",
    "    for i in range(0,len(country_df),int(days_in_sequence/2)):\n",
    "        if i+days_in_sequence<=len(country_df):\n",
    "            \n",
    "            #prepare rate inputs, use rates with noise\n",
    "            removed_rate_trend = [float(x) for x in country_df[i:i+days_in_sequence-1].removed_rate]\n",
    "\n",
    "            #prepare outputs, use original rates\n",
    "            expected_removed_rate = [float(x) for x in country_df.iloc[i+days_in_sequence-1:i+days_in_sequence-1+output_days].removed_rate]\n",
    "\n",
    "            trend_list.append({ \"removed_rate_trend\":removed_rate_trend,\n",
    "                                \"expected_removed_rate\":expected_removed_rate})\n",
    "trend_df = pd.DataFrame(trend_list)\n",
    "\n",
    "trend_df[\"temporal_inputs\"] = [np.asarray([trends[\"removed_rate_trend\"]]) for idx,trends in trend_df.iterrows()]\n",
    "\n",
    "trend_df = shuffle(trend_df)\n",
    "\n",
    "training_item_count = int(len(trend_df)*training_percentage)\n",
    "validation_item_count = len(trend_df)-int(len(trend_df)*training_percentage)\n",
    "training_df = trend_df[:training_item_count]\n",
    "validation_df = trend_df[training_item_count:]\n",
    "\n",
    "X_temporal_train = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in training_df[\"temporal_inputs\"].values]),(training_item_count,temp_dim,sequence_length)),(0,2,1) )).astype(np.float32)\n",
    "Y_removed_train = np.asarray([np.asarray(x) for x in training_df[\"expected_removed_rate\"]]).astype(np.float32)\n",
    "\n",
    "X_temporal_test = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in validation_df[\"temporal_inputs\"]]),(validation_item_count,temp_dim,sequence_length)),(0,2,1)) ).astype(np.float32)\n",
    "Y_removed_test = np.asarray([np.asarray(x) for x in validation_df[\"expected_removed_rate\"]]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3150 samples, validate on 350 samples\n",
      "Epoch 1/50\n",
      "3150/3150 [==============================] - 12s 4ms/step - loss: 0.0012 - val_loss: 6.9939e-04\n",
      "Epoch 2/50\n",
      "3150/3150 [==============================] - 11s 3ms/step - loss: 8.5380e-04 - val_loss: 7.3895e-04\n",
      "Epoch 3/50\n",
      "3150/3150 [==============================] - 11s 3ms/step - loss: 8.0245e-04 - val_loss: 6.8807e-04\n",
      "Epoch 4/50\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 7.5961e-04 - val_loss: 6.5706e-04\n",
      "Epoch 5/50\n",
      "3150/3150 [==============================] - 11s 4ms/step - loss: 7.5433e-04 - val_loss: 6.5417e-04\n",
      "Epoch 6/50\n",
      "3150/3150 [==============================] - 13s 4ms/step - loss: 7.4271e-04 - val_loss: 6.4871e-04\n",
      "Epoch 7/50\n",
      "3150/3150 [==============================] - 12s 4ms/step - loss: 7.2461e-04 - val_loss: 6.8767e-04\n",
      "Epoch 8/50\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 7.2280e-04 - val_loss: 6.7216e-04\n",
      "Epoch 9/50\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 7.3797e-04 - val_loss: 6.8892e-04\n",
      "Epoch 10/50\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 7.0425e-04 - val_loss: 6.6839e-04\n",
      "Epoch 11/50\n",
      "3136/3150 [============================>.] - ETA: 0s - loss: 6.9728e-04\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "3150/3150 [==============================] - 12s 4ms/step - loss: 6.9940e-04 - val_loss: 6.4126e-04\n",
      "Epoch 12/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 6.9402e-04 - val_loss: 6.4189e-04\n",
      "Epoch 13/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 6.8466e-04 - val_loss: 6.4252e-04\n",
      "Epoch 14/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 6.9031e-04 - val_loss: 6.3837e-04\n",
      "Epoch 15/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 2.0686e-04 - val_loss: 2.4711e-05\n",
      "Epoch 16/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 1.0895e-04 - val_loss: 9.5167e-06\n",
      "Epoch 17/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 1.0119e-04 - val_loss: 9.5836e-06\n",
      "Epoch 18/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 9.0791e-05 - val_loss: 7.0403e-06\n",
      "Epoch 19/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 7.8537e-05 - val_loss: 1.6166e-05\n",
      "Epoch 20/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 7.7827e-05 - val_loss: 2.6729e-05\n",
      "Epoch 21/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 7.7985e-05 - val_loss: 6.7064e-06\n",
      "Epoch 22/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 6.9681e-05 - val_loss: 1.1821e-05\n",
      "Epoch 23/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 7.3219e-05 - val_loss: 1.5374e-05\n",
      "Epoch 24/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 8.4521e-05 - val_loss: 1.2958e-05\n",
      "Epoch 25/50\n",
      "3120/3150 [============================>.] - ETA: 0s - loss: 7.5910e-05\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 7.5963e-05 - val_loss: 7.0494e-06\n",
      "Epoch 26/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 6.6461e-05 - val_loss: 2.7750e-05\n",
      "Epoch 27/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 6.2436e-05 - val_loss: 9.5408e-06\n",
      "Epoch 28/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 6.1343e-05 - val_loss: 3.0533e-06\n",
      "Epoch 29/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 6.0890e-05 - val_loss: 1.1337e-05\n",
      "Epoch 30/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 6.5582e-05 - val_loss: 1.2979e-05\n",
      "Epoch 31/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 6.4492e-05 - val_loss: 6.3417e-06\n",
      "Epoch 32/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 7.3604e-05 - val_loss: 1.2088e-05\n",
      "Epoch 33/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 5.9075e-05 - val_loss: 3.7358e-06\n",
      "Epoch 34/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 5.5289e-05 - val_loss: 1.0201e-05\n",
      "Epoch 35/50\n",
      "3136/3150 [============================>.] - ETA: 0s - loss: 6.1155e-05\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.\n",
      "3150/3150 [==============================] - 8s 3ms/step - loss: 6.1036e-05 - val_loss: 2.4837e-05\n",
      "Epoch 36/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 5.0443e-05 - val_loss: 3.4102e-06\n",
      "Epoch 37/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 6.8976e-05 - val_loss: 4.1308e-06\n",
      "Epoch 38/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 6.2024e-05 - val_loss: 4.3258e-06\n",
      "Epoch 39/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 5.6857e-05 - val_loss: 1.1586e-05\n",
      "Epoch 40/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 5.7260e-05 - val_loss: 7.6941e-06\n",
      "Epoch 41/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 6.0243e-05 - val_loss: 2.5982e-06\n",
      "Epoch 42/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 5.6872e-05 - val_loss: 4.1369e-06\n",
      "Epoch 43/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 5.2056e-05 - val_loss: 1.5028e-05\n",
      "Epoch 44/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 6.0013e-05 - val_loss: 1.0971e-05\n",
      "Epoch 45/50\n",
      "3136/3150 [============================>.] - ETA: 0s - loss: 6.4691e-05\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 6.4494e-05 - val_loss: 1.4083e-05\n",
      "Epoch 46/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 6.5252e-05 - val_loss: 5.2643e-06\n",
      "Epoch 47/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.6588e-05 - val_loss: 4.3322e-06\n",
      "Epoch 48/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 5.8135e-05 - val_loss: 5.2429e-06\n",
      "Epoch 49/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 5.6114e-05 - val_loss: 2.1695e-06\n",
      "Epoch 50/50\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 6.0267e-05 - val_loss: 3.6370e-06\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_temporal_train], [Y_removed_train], \n",
    "          epochs = 50, \n",
    "          batch_size = 16, \n",
    "          validation_data=([X_temporal_test],  [Y_removed_test]), \n",
    "          callbacks=callbacks)\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_list = []\n",
    "\n",
    "for i in range(250):\n",
    "    code = \"SIR-small-{}\".format(i)\n",
    "    country_df = train_df.query(f\"Code=='{code}'\")\n",
    "    # Added a quick hack to double the number of sequences\n",
    "    # Warning: This will later create a minor leakage from the training set into the validation set.\n",
    "    for i in range(0,len(country_df),int(days_in_sequence/2)):\n",
    "        if i+days_in_sequence<=len(country_df):\n",
    "            \n",
    "            #prepare rate inputs, use rates with noise\n",
    "            removed_rate_trend = [float(x) for x in country_df[i:i+days_in_sequence-1].removed_noise_rate]\n",
    "\n",
    "            #prepare outputs, use original rates\n",
    "            expected_removed_rate = [float(x) for x in country_df.iloc[i+days_in_sequence-1:i+days_in_sequence-1+output_days].removed_rate]\n",
    "\n",
    "            trend_list.append({ \"removed_rate_trend\":removed_rate_trend,\n",
    "                                \"expected_removed_rate\":expected_removed_rate})\n",
    "trend_df = pd.DataFrame(trend_list)\n",
    "\n",
    "trend_df[\"temporal_inputs\"] = [np.asarray([trends[\"removed_rate_trend\"]]) for idx,trends in trend_df.iterrows()]\n",
    "\n",
    "trend_df = shuffle(trend_df)\n",
    "\n",
    "training_item_count = int(len(trend_df)*training_percentage)\n",
    "validation_item_count = len(trend_df)-int(len(trend_df)*training_percentage)\n",
    "training_df = trend_df[:training_item_count]\n",
    "validation_df = trend_df[training_item_count:]\n",
    "\n",
    "X_temporal_train = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in training_df[\"temporal_inputs\"].values]),(training_item_count,temp_dim,sequence_length)),(0,2,1) )).astype(np.float32)\n",
    "Y_removed_train = np.asarray([np.asarray(x) for x in training_df[\"expected_removed_rate\"]]).astype(np.float32)\n",
    "\n",
    "X_temporal_test = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in validation_df[\"temporal_inputs\"]]),(validation_item_count,temp_dim,sequence_length)),(0,2,1)) ).astype(np.float32)\n",
    "Y_removed_test = np.asarray([np.asarray(x) for x in validation_df[\"expected_removed_rate\"]]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3150 samples, validate on 350 samples\n",
      "Epoch 51/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 5.6160e-05 - val_loss: 7.5886e-06\n",
      "Epoch 52/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 5.5330e-05 - val_loss: 6.1686e-06\n",
      "Epoch 53/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.9486e-05 - val_loss: 7.9211e-06\n",
      "Epoch 54/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 5.2226e-05 - val_loss: 6.4614e-06\n",
      "Epoch 55/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 5.1572e-05 - val_loss: 6.9652e-06\n",
      "Epoch 56/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.9676e-05 - val_loss: 5.5117e-06\n",
      "Epoch 57/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.6895e-05 - val_loss: 7.8453e-06\n",
      "Epoch 58/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 5.4497e-05 - val_loss: 7.1848e-06\n",
      "Epoch 59/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 5.4466e-05 - val_loss: 1.3426e-05\n",
      "Epoch 60/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 5.9207e-05 - val_loss: 1.5268e-05\n",
      "Epoch 61/100\n",
      "3136/3150 [============================>.] - ETA: 0s - loss: 5.7547e-05\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 5.7355e-05 - val_loss: 5.9936e-06\n",
      "Epoch 62/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.9271e-05 - val_loss: 6.6847e-06\n",
      "Epoch 63/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.9596e-05 - val_loss: 7.3021e-06\n",
      "Epoch 64/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 5.3393e-05 - val_loss: 8.2331e-06\n",
      "Epoch 65/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 5.2785e-05 - val_loss: 6.1772e-06\n",
      "Epoch 66/100\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 5.1122e-05 - val_loss: 6.6506e-06\n",
      "Epoch 67/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.9762e-05 - val_loss: 7.2175e-06\n",
      "Epoch 68/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.5715e-05 - val_loss: 6.0172e-06\n",
      "Epoch 69/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 5.3416e-05 - val_loss: 1.0370e-05\n",
      "Epoch 70/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 5.1642e-05 - val_loss: 8.5116e-06\n",
      "Epoch 71/100\n",
      "3120/3150 [============================>.] - ETA: 0s - loss: 5.1354e-05\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 5.0964e-05 - val_loss: 7.1257e-06\n",
      "Epoch 72/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.6416e-05 - val_loss: 7.6481e-06\n",
      "Epoch 73/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.0902e-05 - val_loss: 8.0595e-06\n",
      "Epoch 74/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.8521e-05 - val_loss: 5.2959e-06\n",
      "Epoch 75/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.5961e-05 - val_loss: 7.4249e-06\n",
      "Epoch 76/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 5.5443e-05 - val_loss: 7.2161e-06\n",
      "Epoch 77/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.9603e-05 - val_loss: 8.0457e-06\n",
      "Epoch 78/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 5.3529e-05 - val_loss: 5.2842e-06\n",
      "Epoch 79/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.8583e-05 - val_loss: 6.1420e-06\n",
      "Epoch 80/100\n",
      "3150/3150 [==============================] - 8s 3ms/step - loss: 4.9851e-05 - val_loss: 5.1996e-06\n",
      "Epoch 81/100\n",
      "3120/3150 [============================>.] - ETA: 0s - loss: 4.9534e-05\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 2.799360081553459e-05.\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.9455e-05 - val_loss: 7.9586e-06\n",
      "Epoch 82/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.9929e-05 - val_loss: 5.2662e-06\n",
      "Epoch 83/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.9299e-05 - val_loss: 6.1059e-06\n",
      "Epoch 84/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.9194e-05 - val_loss: 5.4345e-06\n",
      "Epoch 85/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 5.0645e-05 - val_loss: 6.9385e-06\n",
      "Epoch 86/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.8475e-05 - val_loss: 6.2134e-06\n",
      "Epoch 87/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.6931e-05 - val_loss: 5.2756e-06\n",
      "Epoch 88/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 5.1286e-05 - val_loss: 6.4635e-06\n",
      "Epoch 89/100\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 4.8229e-05 - val_loss: 6.4562e-06\n",
      "Epoch 90/100\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 5.8119e-05 - val_loss: 6.3839e-06\n",
      "Epoch 91/100\n",
      "3136/3150 [============================>.] - ETA: 0s - loss: 4.5282e-05\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.5147e-05 - val_loss: 5.8976e-06\n",
      "Epoch 92/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.8636e-05 - val_loss: 5.4393e-06\n",
      "Epoch 93/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.5922e-05 - val_loss: 6.4522e-06\n",
      "Epoch 94/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.7783e-05 - val_loss: 5.7202e-06\n",
      "Epoch 95/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.9985e-05 - val_loss: 5.7605e-06\n",
      "Epoch 96/100\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.8667e-05 - val_loss: 5.5988e-06\n",
      "Epoch 97/100\n",
      "3150/3150 [==============================] - 12s 4ms/step - loss: 4.6877e-05 - val_loss: 6.4120e-06\n",
      "Epoch 98/100\n",
      "3150/3150 [==============================] - 15s 5ms/step - loss: 4.3492e-05 - val_loss: 5.4110e-06\n",
      "Epoch 99/100\n",
      "3150/3150 [==============================] - 12s 4ms/step - loss: 4.7549e-05 - val_loss: 5.9890e-06\n",
      "Epoch 100/100\n",
      "3150/3150 [==============================] - 12s 4ms/step - loss: 4.9786e-05 - val_loss: 5.9728e-06\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_temporal_train], [Y_removed_train], \n",
    "          initial_epoch=50,\n",
    "          epochs = 100,  \n",
    "          batch_size = 16, \n",
    "          validation_data=([X_temporal_test],  [Y_removed_test]), \n",
    "          callbacks=callbacks)\n",
    "loss.extend(history.history['loss'])\n",
    "val_loss.extend(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_list = []\n",
    "\n",
    "for i in range(250):\n",
    "    code = \"SIR-middle-{}\".format(i)\n",
    "    country_df = train_df.query(f\"Code=='{code}'\")\n",
    "    # Added a quick hack to double the number of sequences\n",
    "    # Warning: This will later create a minor leakage from the training set into the validation set.\n",
    "    for i in range(0,len(country_df),int(days_in_sequence/2)):\n",
    "        if i+days_in_sequence<=len(country_df):\n",
    "            \n",
    "            #prepare rate inputs, use rates with noise\n",
    "            removed_rate_trend = [float(x) for x in country_df[i:i+days_in_sequence-1].removed_noise_rate]\n",
    "\n",
    "            #prepare outputs, use original rates\n",
    "            expected_removed_rate = [float(x) for x in country_df.iloc[i+days_in_sequence-1:i+days_in_sequence-1+output_days].removed_rate]\n",
    "\n",
    "            trend_list.append({ \"removed_rate_trend\":removed_rate_trend,\n",
    "                                \"expected_removed_rate\":expected_removed_rate})\n",
    "trend_df = pd.DataFrame(trend_list)\n",
    "\n",
    "trend_df[\"temporal_inputs\"] = [np.asarray([trends[\"removed_rate_trend\"]]) for idx,trends in trend_df.iterrows()]\n",
    "\n",
    "trend_df = shuffle(trend_df)\n",
    "\n",
    "training_item_count = int(len(trend_df)*training_percentage)\n",
    "validation_item_count = len(trend_df)-int(len(trend_df)*training_percentage)\n",
    "training_df = trend_df[:training_item_count]\n",
    "validation_df = trend_df[training_item_count:]\n",
    "\n",
    "X_temporal_train = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in training_df[\"temporal_inputs\"].values]),(training_item_count,temp_dim,sequence_length)),(0,2,1) )).astype(np.float32)\n",
    "Y_removed_train = np.asarray([np.asarray(x) for x in training_df[\"expected_removed_rate\"]]).astype(np.float32)\n",
    "\n",
    "X_temporal_test = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in validation_df[\"temporal_inputs\"]]),(validation_item_count,temp_dim,sequence_length)),(0,2,1)) ).astype(np.float32)\n",
    "Y_removed_test = np.asarray([np.asarray(x) for x in validation_df[\"expected_removed_rate\"]]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3150 samples, validate on 350 samples\n",
      "Epoch 101/150\n",
      "3150/3150 [==============================] - 11s 3ms/step - loss: 2.7256e-05 - val_loss: 1.1719e-06\n",
      "Epoch 102/150\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 2.6659e-05 - val_loss: 1.2253e-06\n",
      "Epoch 103/150\n",
      "3150/3150 [==============================] - 11s 3ms/step - loss: 2.3736e-05 - val_loss: 1.0932e-06\n",
      "Epoch 104/150\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 2.5235e-05 - val_loss: 1.6390e-06\n",
      "Epoch 105/150\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 2.4672e-05 - val_loss: 1.8008e-06\n",
      "Epoch 106/150\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 2.5146e-05 - val_loss: 1.8904e-06\n",
      "Epoch 107/150\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 2.9589e-05 - val_loss: 1.1097e-06\n",
      "Epoch 108/150\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 2.5590e-05 - val_loss: 1.5870e-06\n",
      "Epoch 109/150\n",
      "3150/3150 [==============================] - 11s 3ms/step - loss: 2.6418e-05 - val_loss: 1.3845e-06\n",
      "Epoch 110/150\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 2.4389e-05 - val_loss: 1.2021e-06\n",
      "Epoch 111/150\n",
      "3136/3150 [============================>.] - ETA: 0s - loss: 2.4332e-05\n",
      "Epoch 00111: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 2.4263e-05 - val_loss: 1.1038e-06\n",
      "Epoch 112/150\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 2.4729e-05 - val_loss: 2.2496e-06\n",
      "Epoch 113/150\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 2.5515e-05 - val_loss: 1.1776e-06\n",
      "Epoch 114/150\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 2.5222e-05 - val_loss: 2.2807e-06\n",
      "Epoch 115/150\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 2.3547e-05 - val_loss: 1.5833e-06\n",
      "Epoch 116/150\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 2.3261e-05 - val_loss: 1.5421e-06\n",
      "Epoch 117/150\n",
      "3150/3150 [==============================] - 11s 3ms/step - loss: 2.8982e-05 - val_loss: 1.4631e-06\n",
      "Epoch 118/150\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 2.3473e-05 - val_loss: 1.5489e-06\n",
      "Epoch 119/150\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 2.8376e-05 - val_loss: 1.5122e-06\n",
      "Epoch 120/150\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 2.8398e-05 - val_loss: 1.4851e-06\n",
      "Epoch 121/150\n",
      "3120/3150 [============================>.] - ETA: 0s - loss: 2.5251e-05\n",
      "Epoch 00121: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 2.5244e-05 - val_loss: 1.4861e-06\n",
      "Epoch 122/150\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 2.6300e-05 - val_loss: 1.4027e-06\n",
      "Epoch 123/150\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 2.6094e-05 - val_loss: 1.2575e-06\n",
      "Epoch 124/150\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 2.2919e-05 - val_loss: 1.5651e-06\n",
      "Epoch 125/150\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 2.3166e-05 - val_loss: 1.1461e-06\n",
      "Epoch 126/150\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 2.4405e-05 - val_loss: 1.3795e-06\n",
      "Epoch 127/150\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 2.3480e-05 - val_loss: 1.3236e-06\n",
      "Epoch 128/150\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 2.6356e-05 - val_loss: 1.3828e-06\n",
      "Epoch 129/150\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 2.6429e-05 - val_loss: 1.0769e-06\n",
      "Epoch 130/150\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 2.2134e-05 - val_loss: 1.4014e-06\n",
      "Epoch 131/150\n",
      "3136/3150 [============================>.] - ETA: 0s - loss: 2.5492e-05\n",
      "Epoch 00131: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 2.5417e-05 - val_loss: 1.1645e-06\n",
      "Epoch 132/150\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 2.3208e-05 - val_loss: 1.2430e-06\n",
      "Epoch 133/150\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 2.7499e-05 - val_loss: 1.4395e-06\n",
      "Epoch 134/150\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 2.9113e-05 - val_loss: 1.3893e-06\n",
      "Epoch 135/150\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 2.7734e-05 - val_loss: 1.4065e-06\n",
      "Epoch 136/150\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 2.2847e-05 - val_loss: 1.6582e-06\n",
      "Epoch 137/150\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 2.3823e-05 - val_loss: 1.9395e-06\n",
      "Epoch 138/150\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 2.7634e-05 - val_loss: 1.2552e-06\n",
      "Epoch 139/150\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 2.7601e-05 - val_loss: 1.2810e-06\n",
      "Epoch 140/150\n",
      "3150/3150 [==============================] - 13s 4ms/step - loss: 2.2068e-05 - val_loss: 1.6208e-06\n",
      "Epoch 141/150\n",
      "3136/3150 [============================>.] - ETA: 0s - loss: 2.5166e-05\n",
      "Epoch 00141: ReduceLROnPlateau reducing learning rate to 2.1767824364360423e-06.\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 2.5094e-05 - val_loss: 1.6425e-06\n",
      "Epoch 142/150\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 2.5656e-05 - val_loss: 1.3871e-06\n",
      "Epoch 143/150\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 2.5784e-05 - val_loss: 1.6514e-06\n",
      "Epoch 144/150\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 2.5759e-05 - val_loss: 1.4112e-06\n",
      "Epoch 145/150\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 2.8712e-05 - val_loss: 1.2583e-06\n",
      "Epoch 146/150\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 2.7561e-05 - val_loss: 1.4144e-06\n",
      "Epoch 147/150\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 2.5161e-05 - val_loss: 1.2807e-06\n",
      "Epoch 148/150\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 2.2648e-05 - val_loss: 1.2327e-06\n",
      "Epoch 149/150\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 2.3900e-05 - val_loss: 1.5577e-06\n",
      "Epoch 150/150\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 2.2426e-05 - val_loss: 1.2523e-06\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_temporal_train], [Y_removed_train],\n",
    "          initial_epoch=100,\n",
    "          epochs = 150, \n",
    "          batch_size = 16, \n",
    "          validation_data=([X_temporal_test],  [Y_removed_test]), \n",
    "          callbacks=callbacks)\n",
    "loss.extend(history.history['loss'])\n",
    "val_loss.extend(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_list = []\n",
    "\n",
    "for i in range(250):\n",
    "    code = \"SIR-large-{}\".format(i)\n",
    "    country_df = train_df.query(f\"Code=='{code}'\")\n",
    "    # Added a quick hack to double the number of sequences\n",
    "    # Warning: This will later create a minor leakage from the training set into the validation set.\n",
    "    for i in range(0,len(country_df),int(days_in_sequence/2)):\n",
    "        if i+days_in_sequence<=len(country_df):\n",
    "            \n",
    "            #prepare rate inputs, use rates with noise\n",
    "            removed_rate_trend = [float(x) for x in country_df[i:i+days_in_sequence-1].removed_noise_rate]\n",
    "\n",
    "            #prepare outputs, use original rates\n",
    "            expected_removed_rate = [float(x) for x in country_df.iloc[i+days_in_sequence-1:i+days_in_sequence-1+output_days].removed_rate]\n",
    "\n",
    "            trend_list.append({ \"removed_rate_trend\":removed_rate_trend,\n",
    "                                \"expected_removed_rate\":expected_removed_rate})\n",
    "trend_df = pd.DataFrame(trend_list)\n",
    "\n",
    "trend_df[\"temporal_inputs\"] = [np.asarray([trends[\"removed_rate_trend\"]]) for idx,trends in trend_df.iterrows()]\n",
    "\n",
    "trend_df = shuffle(trend_df)\n",
    "\n",
    "training_item_count = int(len(trend_df)*training_percentage)\n",
    "validation_item_count = len(trend_df)-int(len(trend_df)*training_percentage)\n",
    "training_df = trend_df[:training_item_count]\n",
    "validation_df = trend_df[training_item_count:]\n",
    "\n",
    "X_temporal_train = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in training_df[\"temporal_inputs\"].values]),(training_item_count,temp_dim,sequence_length)),(0,2,1) )).astype(np.float32)\n",
    "Y_removed_train = np.asarray([np.asarray(x) for x in training_df[\"expected_removed_rate\"]]).astype(np.float32)\n",
    "\n",
    "X_temporal_test = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in validation_df[\"temporal_inputs\"]]),(validation_item_count,temp_dim,sequence_length)),(0,2,1)) ).astype(np.float32)\n",
    "Y_removed_test = np.asarray([np.asarray(x) for x in validation_df[\"expected_removed_rate\"]]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3150 samples, validate on 350 samples\n",
      "Epoch 151/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.4886e-05 - val_loss: 4.5472e-06\n",
      "Epoch 152/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.6625e-05 - val_loss: 3.9831e-06\n",
      "Epoch 153/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.3289e-05 - val_loss: 4.0344e-06\n",
      "Epoch 154/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.4813e-05 - val_loss: 3.8949e-06\n",
      "Epoch 155/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.8915e-05 - val_loss: 3.8646e-06\n",
      "Epoch 156/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.5542e-05 - val_loss: 3.9793e-06\n",
      "Epoch 157/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.5643e-05 - val_loss: 3.8719e-06\n",
      "Epoch 158/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.9677e-05 - val_loss: 3.7866e-06\n",
      "Epoch 159/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.2490e-05 - val_loss: 3.7781e-06\n",
      "Epoch 160/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.9402e-05 - val_loss: 3.6911e-06\n",
      "Epoch 161/200\n",
      "3136/3150 [============================>.] - ETA: 0s - loss: 3.5261e-05\n",
      "Epoch 00161: ReduceLROnPlateau reducing learning rate to 1.3060694072919432e-06.\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.5172e-05 - val_loss: 3.7694e-06\n",
      "Epoch 162/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.5689e-05 - val_loss: 3.9359e-06\n",
      "Epoch 163/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.9625e-05 - val_loss: 3.7701e-06\n",
      "Epoch 164/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 5.5253e-05 - val_loss: 3.6461e-06\n",
      "Epoch 165/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.1119e-05 - val_loss: 3.7050e-06\n",
      "Epoch 166/200\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 3.1385e-05 - val_loss: 3.6734e-06\n",
      "Epoch 167/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.5060e-05 - val_loss: 3.7747e-06\n",
      "Epoch 168/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.8795e-05 - val_loss: 3.6976e-06\n",
      "Epoch 169/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.6884e-05 - val_loss: 3.8120e-06\n",
      "Epoch 170/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.8578e-05 - val_loss: 3.8126e-06\n",
      "Epoch 171/200\n",
      "3136/3150 [============================>.] - ETA: 0s - loss: 3.1637e-05\n",
      "Epoch 00171: ReduceLROnPlateau reducing learning rate to 7.836416443751659e-07.\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.1524e-05 - val_loss: 3.7801e-06\n",
      "Epoch 172/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.1967e-05 - val_loss: 3.6955e-06\n",
      "Epoch 173/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.3864e-05 - val_loss: 3.6897e-06\n",
      "Epoch 174/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.1931e-05 - val_loss: 3.8070e-06\n",
      "Epoch 175/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.0720e-05 - val_loss: 3.8178e-06\n",
      "Epoch 176/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.2055e-05 - val_loss: 3.8289e-06\n",
      "Epoch 177/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.6105e-05 - val_loss: 3.8211e-06\n",
      "Epoch 178/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.2410e-05 - val_loss: 3.6749e-06\n",
      "Epoch 179/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.2466e-05 - val_loss: 3.7148e-06\n",
      "Epoch 180/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.7729e-05 - val_loss: 3.7578e-06\n",
      "Epoch 181/200\n",
      "3120/3150 [============================>.] - ETA: 0s - loss: 3.2616e-05\n",
      "Epoch 00181: ReduceLROnPlateau reducing learning rate to 4.701850002675201e-07.\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.2405e-05 - val_loss: 3.7039e-06\n",
      "Epoch 182/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.0471e-05 - val_loss: 3.6215e-06\n",
      "Epoch 183/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.4570e-05 - val_loss: 3.6298e-06\n",
      "Epoch 184/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.9260e-05 - val_loss: 3.6910e-06\n",
      "Epoch 185/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.4215e-05 - val_loss: 3.6538e-06\n",
      "Epoch 186/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.1647e-05 - val_loss: 3.6812e-06\n",
      "Epoch 187/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.9929e-05 - val_loss: 3.8067e-06\n",
      "Epoch 188/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.0995e-05 - val_loss: 3.7554e-06\n",
      "Epoch 189/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.6549e-05 - val_loss: 3.7109e-06\n",
      "Epoch 190/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.0492e-05 - val_loss: 3.7167e-06\n",
      "Epoch 191/200\n",
      "3136/3150 [============================>.] - ETA: 0s - loss: 4.1889e-05\n",
      "Epoch 00191: ReduceLROnPlateau reducing learning rate to 2.8211100016051206e-07.\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 4.1862e-05 - val_loss: 3.7055e-06\n",
      "Epoch 192/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.7084e-05 - val_loss: 3.6916e-06\n",
      "Epoch 193/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.3496e-05 - val_loss: 3.7333e-06\n",
      "Epoch 194/200\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 3.1867e-05 - val_loss: 3.7554e-06\n",
      "Epoch 195/200\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 3.7199e-05 - val_loss: 3.7703e-06\n",
      "Epoch 196/200\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 3.6160e-05 - val_loss: 3.7530e-06\n",
      "Epoch 197/200\n",
      "3150/3150 [==============================] - 10s 3ms/step - loss: 3.1000e-05 - val_loss: 3.7107e-06\n",
      "Epoch 198/200\n",
      "3150/3150 [==============================] - 12s 4ms/step - loss: 3.9858e-05 - val_loss: 3.7166e-06\n",
      "Epoch 199/200\n",
      "3150/3150 [==============================] - 9s 3ms/step - loss: 3.7543e-05 - val_loss: 3.7029e-06\n",
      "Epoch 200/200\n",
      "3150/3150 [==============================] - 8s 3ms/step - loss: 3.8079e-05 - val_loss: 3.6612e-06\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_temporal_train], [Y_removed_train],\n",
    "          initial_epoch=150,\n",
    "          epochs = 200, \n",
    "          batch_size = 16, \n",
    "          validation_data=([X_temporal_test],  [Y_removed_test]), \n",
    "          callbacks=callbacks)\n",
    "loss.extend(history.history['loss'])\n",
    "val_loss.extend(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Performance during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.title('Loss over epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Generate predictions using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"best_removed_model.h5\")\n",
    "predictions = model.predict([X_temporal_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "[[0.08901121]\n",
      " [0.08904027]\n",
      " [0.08906324]\n",
      " [0.08908131]\n",
      " [0.08909543]\n",
      " [0.0891064 ]\n",
      " [0.08911486]\n",
      " [0.08912136]\n",
      " [0.08912632]\n",
      " [0.08913007]\n",
      " [0.0891329 ]\n",
      " [0.08913501]\n",
      " [0.08913658]\n",
      " [0.08913773]\n",
      " [0.08913856]\n",
      " [0.08913917]\n",
      " [0.08913961]\n",
      " [0.08913992]\n",
      " [0.08914014]\n",
      " [0.0891403 ]]\n",
      "Expected removed rate: [0.08914041 0.08914048 0.08914053 0.08914057 0.08914059 0.08914061\n",
      " 0.08914062]\n",
      "Prediction: [0.08868365 0.08894974 0.08911716 0.08939055 0.0899293  0.08971497\n",
      " 0.08993942]\n",
      "================================================\n",
      "[[0.01395797]\n",
      " [0.01461757]\n",
      " [0.01525965]\n",
      " [0.01588254]\n",
      " [0.01648476]\n",
      " [0.01706497]\n",
      " [0.017622  ]\n",
      " [0.01815488]\n",
      " [0.01866283]\n",
      " [0.01914525]\n",
      " [0.01960174]\n",
      " [0.02003207]\n",
      " [0.02043051]\n",
      " [0.02079801]\n",
      " [0.02113563]\n",
      " [0.02144456]\n",
      " [0.02172612]\n",
      " [0.02198168]\n",
      " [0.02221268]\n",
      " [0.02242061]]\n",
      "Expected removed rate: [0.02260698 0.02277331 0.0229196  0.02304769 0.02315934 0.02325622\n",
      " 0.02333989]\n",
      "Prediction: [0.02254074 0.02285709 0.02321348 0.02353872 0.02378734 0.02406138\n",
      " 0.02434178]\n",
      "================================================\n",
      "[[0.00014045]\n",
      " [0.00017523]\n",
      " [0.00021612]\n",
      " [0.00026405]\n",
      " [0.00032004]\n",
      " [0.00038526]\n",
      " [0.00046099]\n",
      " [0.00054868]\n",
      " [0.00064991]\n",
      " [0.00076646]\n",
      " [0.00090027]\n",
      " [0.00105345]\n",
      " [0.00122489]\n",
      " [0.00141625]\n",
      " [0.00162924]\n",
      " [0.00186567]\n",
      " [0.00212741]\n",
      " [0.00241636]\n",
      " [0.0027345 ]\n",
      " [0.00308382]]\n",
      "Expected removed rate: [0.00346632 0.003884   0.0043343  0.0048184  0.00533739 0.00589223\n",
      " 0.00648371]\n",
      "Prediction: [0.00336312 0.00364331 0.00391476 0.00411371 0.00446831 0.00474183\n",
      " 0.00502968]\n",
      "================================================\n",
      "[[0.01237221]\n",
      " [0.01239084]\n",
      " [0.01240642]\n",
      " [0.01241939]\n",
      " [0.01243013]\n",
      " [0.01243898]\n",
      " [0.01244622]\n",
      " [0.01245213]\n",
      " [0.01245692]\n",
      " [0.01246078]\n",
      " [0.01246387]\n",
      " [0.01246633]\n",
      " [0.01246828]\n",
      " [0.0124698 ]\n",
      " [0.012471  ]\n",
      " [0.01247192]\n",
      " [0.01247263]\n",
      " [0.01247317]\n",
      " [0.01247359]\n",
      " [0.0124739 ]]\n",
      "Expected removed rate: [0.01247413 0.01247431 0.01247444 0.01247453 0.0124746  0.01247465\n",
      " 0.01247469]\n",
      "Prediction: [0.01208444 0.01219933 0.01236365 0.01249501 0.0126857  0.01266225\n",
      " 0.01279187]\n",
      "================================================\n",
      "[[0.00106978]\n",
      " [0.00117553]\n",
      " [0.00128562]\n",
      " [0.00139992]\n",
      " [0.00151826]\n",
      " [0.00164042]\n",
      " [0.00176619]\n",
      " [0.00189528]\n",
      " [0.00202742]\n",
      " [0.00216227]\n",
      " [0.0022995 ]\n",
      " [0.00243872]\n",
      " [0.00257762]\n",
      " [0.00271577]\n",
      " [0.00285275]\n",
      " [0.00298816]\n",
      " [0.00312159]\n",
      " [0.00325265]\n",
      " [0.00338096]\n",
      " [0.00350619]]\n",
      "Expected removed rate: [0.003628   0.0037461  0.00385906 0.00396675 0.00406905 0.00416589\n",
      " 0.00425723]\n",
      "Prediction: [0.0036018  0.00380091 0.00404137 0.00421504 0.00445415 0.00461919\n",
      " 0.00483098]\n",
      "================================================\n",
      "[[0.01118559]\n",
      " [0.01165692]\n",
      " [0.01211236]\n",
      " [0.01255097]\n",
      " [0.0129719 ]\n",
      " [0.01337447]\n",
      " [0.0137581 ]\n",
      " [0.0141224 ]\n",
      " [0.01446708]\n",
      " [0.014792  ]\n",
      " [0.01509716]\n",
      " [0.0153827 ]\n",
      " [0.01564565]\n",
      " [0.01588687]\n",
      " [0.01610728]\n",
      " [0.01630787]\n",
      " [0.01648969]\n",
      " [0.01665382]\n",
      " [0.01680136]\n",
      " [0.01693343]]\n",
      "Expected removed rate: [0.01705115 0.01715564 0.01724766 0.01732836 0.0173988  0.01746001\n",
      " 0.01751296]\n",
      "Prediction: [0.01687388 0.01711435 0.01740598 0.01766074 0.01787452 0.01805099\n",
      " 0.01827513]\n",
      "================================================\n",
      "[[0.00031319]\n",
      " [0.00036381]\n",
      " [0.00041984]\n",
      " [0.00048167]\n",
      " [0.00054974]\n",
      " [0.00062446]\n",
      " [0.00070626]\n",
      " [0.00079558]\n",
      " [0.00089283]\n",
      " [0.00099845]\n",
      " [0.00111282]\n",
      " [0.00123636]\n",
      " [0.00136824]\n",
      " [0.00150865]\n",
      " [0.00165772]\n",
      " [0.00181554]\n",
      " [0.00198217]\n",
      " [0.0021576 ]\n",
      " [0.00234176]\n",
      " [0.00253455]]\n",
      "Expected removed rate: [0.00273578 0.00294521 0.00315997 0.00337954 0.00360334 0.00383078\n",
      " 0.00406118]\n",
      "Prediction: [0.00267789 0.00289564 0.00313815 0.00331069 0.00358735 0.00377809\n",
      " 0.00400971]\n",
      "================================================\n",
      "[[0.00496794]\n",
      " [0.00627628]\n",
      " [0.00780882]\n",
      " [0.00959776]\n",
      " [0.01167859]\n",
      " [0.01409015]\n",
      " [0.01687459]\n",
      " [0.02007722]\n",
      " [0.02374631]\n",
      " [0.02793262]\n",
      " [0.03268894]\n",
      " [0.03806931]\n",
      " [0.04401441]\n",
      " [0.05055626]\n",
      " [0.05772426]\n",
      " [0.06554435]\n",
      " [0.07403819]\n",
      " [0.08322232]\n",
      " [0.09310737]\n",
      " [0.1036973 ]]\n",
      "Expected removed rate: [0.11498877 0.12697062 0.13934124 0.15206057 0.16508435 0.17836449\n",
      " 0.19184968]\n",
      "Prediction: [0.11959103 0.1254463  0.12909523 0.13244545 0.1383886  0.14622903\n",
      " 0.15137601]\n",
      "================================================\n",
      "[[0.01878382]\n",
      " [0.0196446 ]\n",
      " [0.02048665]\n",
      " [0.02130756]\n",
      " [0.02210514]\n",
      " [0.02287736]\n",
      " [0.02362241]\n",
      " [0.02433869]\n",
      " [0.02502486]\n",
      " [0.02567977]\n",
      " [0.02630257]\n",
      " [0.02689263]\n",
      " [0.02744589]\n",
      " [0.02796268]\n",
      " [0.02844351]\n",
      " [0.02888913]\n",
      " [0.02930048]\n",
      " [0.02967865]\n",
      " [0.03002492]\n",
      " [0.03034065]]\n",
      "Expected removed rate: [0.03062734 0.03088654 0.03111669 0.03132013 0.03149916 0.031656\n",
      " 0.03179276]\n",
      "Prediction: [0.03065572 0.03106696 0.03149801 0.03190761 0.03221327 0.03259919\n",
      " 0.03295059]\n",
      "================================================\n",
      "[[2.1251135e-07]\n",
      " [3.5859614e-07]\n",
      " [5.7478798e-07]\n",
      " [8.8935616e-07]\n",
      " [1.3405596e-06]\n",
      " [1.9798460e-06]\n",
      " [2.8759969e-06]\n",
      " [4.1204789e-06]\n",
      " [5.8343267e-06]\n",
      " [8.1769676e-06]\n",
      " [1.1357498e-05]\n",
      " [1.5649042e-05]\n",
      " [2.1168375e-05]\n",
      " [2.8229540e-05]\n",
      " [3.7219172e-05]\n",
      " [4.8611702e-05]\n",
      " [6.2987383e-05]\n",
      " [8.1053659e-05]\n",
      " [1.0367031e-04]\n",
      " [1.3187906e-04]]\n",
      "Expected removed rate: [0.00016694 0.00021036 0.0002621  0.00032355 0.0003963  0.00048216\n",
      " 0.00058321]\n",
      "Prediction: [-1.0697916e-05 -3.7178398e-07  1.6838312e-04  2.7327985e-04\n",
      "  4.4281781e-04  4.7602504e-04  6.1023608e-04]\n",
      "================================================\n",
      "[[0.06268658]\n",
      " [0.06268664]\n",
      " [0.06268667]\n",
      " [0.0626867 ]\n",
      " [0.06268672]\n",
      " [0.06268673]\n",
      " [0.06268674]\n",
      " [0.06268675]\n",
      " [0.06268675]\n",
      " [0.06268675]\n",
      " [0.06268676]\n",
      " [0.06268676]\n",
      " [0.06268676]\n",
      " [0.06268676]\n",
      " [0.06268676]\n",
      " [0.06268676]\n",
      " [0.06268676]\n",
      " [0.06268676]\n",
      " [0.06268676]\n",
      " [0.06268676]]\n",
      "Expected removed rate: [0.06268676 0.06268676 0.06268676 0.06268676 0.06268676 0.06268676\n",
      " 0.06268676]\n",
      "Prediction: [0.06216135 0.06237328 0.06253068 0.06276079 0.06316283 0.06299894\n",
      " 0.06318028]\n",
      "================================================\n",
      "[[0.00134419]\n",
      " [0.00148942]\n",
      " [0.00164202]\n",
      " [0.00180191]\n",
      " [0.00196897]\n",
      " [0.00214304]\n",
      " [0.0023239 ]\n",
      " [0.00251127]\n",
      " [0.00270483]\n",
      " [0.00290422]\n",
      " [0.003109  ]\n",
      " [0.00331871]\n",
      " [0.00352951]\n",
      " [0.00374077]\n",
      " [0.00395184]\n",
      " [0.00416208]\n",
      " [0.00437082]\n",
      " [0.00457742]\n",
      " [0.00478126]\n",
      " [0.00498173]]\n",
      "Expected removed rate: [0.00517822 0.0053702  0.00555537 0.00573336 0.00590386 0.00606663\n",
      " 0.00622146]\n",
      "Prediction: [0.00527664 0.00553403 0.0058129  0.00602392 0.00631247 0.00655824\n",
      " 0.00681879]\n",
      "================================================\n",
      "[[8.7843136e-06]\n",
      " [1.2119232e-05]\n",
      " [1.6348260e-05]\n",
      " [2.1682965e-05]\n",
      " [2.8379558e-05]\n",
      " [3.6747304e-05]\n",
      " [4.7158290e-05]\n",
      " [6.0058777e-05]\n",
      " [7.5982323e-05]\n",
      " [9.5564894e-05]\n",
      " [1.1956221e-04]\n",
      " [1.4886957e-04]\n",
      " [1.8329953e-04]\n",
      " [2.2361777e-04]\n",
      " [2.7068399e-04]\n",
      " [3.2546042e-04]\n",
      " [3.8902037e-04]\n",
      " [4.6255687e-04]\n",
      " [5.4739148e-04]\n",
      " [6.4498297e-04]]\n",
      "Expected removed rate: [0.00075694 0.00088501 0.00102844 0.00118863 0.00136706 0.00156526\n",
      " 0.00178482]\n",
      "Prediction: [0.00049707 0.00063451 0.00082478 0.0009468  0.00115411 0.00123329\n",
      " 0.00139746]\n",
      "================================================\n",
      "[[0.0063522]\n",
      " [0.0063522]\n",
      " [0.0063522]\n",
      " [0.0063522]\n",
      " [0.0063522]\n",
      " [0.0063522]\n",
      " [0.0063522]\n",
      " [0.0063522]\n",
      " [0.0063522]\n",
      " [0.0063522]\n",
      " [0.0063522]\n",
      " [0.0063522]\n",
      " [0.0063522]\n",
      " [0.0063522]\n",
      " [0.0063522]\n",
      " [0.0063522]\n",
      " [0.0063522]\n",
      " [0.0063522]\n",
      " [0.0063522]\n",
      " [0.0063522]]\n",
      "Expected removed rate: [0.0063522 0.0063522 0.0063522 0.0063522 0.0063522 0.0063522 0.0063522]\n",
      "Prediction: [0.00603208 0.00613713 0.00630268 0.00641896 0.00659459 0.00659284\n",
      " 0.00672085]\n",
      "================================================\n",
      "[[1.1760404e-07]\n",
      " [1.8554482e-07]\n",
      " [2.7957216e-07]\n",
      " [4.0751308e-07]\n",
      " [5.7912087e-07]\n",
      " [8.0648363e-07]\n",
      " [1.1045087e-06]\n",
      " [1.4914956e-06]\n",
      " [1.9898102e-06]\n",
      " [2.6266764e-06]\n",
      " [3.4351028e-06]\n",
      " [4.4549624e-06]\n",
      " [5.6796821e-06]\n",
      " [7.1426457e-06]\n",
      " [8.8816087e-06]\n",
      " [1.0939125e-05]\n",
      " [1.3363002e-05]\n",
      " [1.6206770e-05]\n",
      " [1.9530169e-05]\n",
      " [2.3399651e-05]]\n",
      "Expected removed rate: [2.7888886e-05 3.3079265e-05 3.8960599e-05 4.5603341e-05 5.3082513e-05\n",
      " 6.1477629e-05 7.0872637e-05]\n",
      "Prediction: [-2.2200868e-05 -1.2622401e-05  4.2371452e-05  1.4394149e-04\n",
      "  3.0427799e-04  3.2774359e-04  4.5529380e-04]\n",
      "================================================\n",
      "[[0.01409021]\n",
      " [0.0152217 ]\n",
      " [0.01637355]\n",
      " [0.01754256]\n",
      " [0.01872533]\n",
      " [0.0199183 ]\n",
      " [0.0211178 ]\n",
      " [0.02232002]\n",
      " [0.02352113]\n",
      " [0.02471725]\n",
      " [0.02590449]\n",
      " [0.02707903]\n",
      " [0.0282251 ]\n",
      " [0.02933959]\n",
      " [0.03041966]\n",
      " [0.03146272]\n",
      " [0.03246651]\n",
      " [0.03342906]\n",
      " [0.03434874]\n",
      " [0.03522426]]\n",
      "Expected removed rate: [0.03605467 0.03683936 0.03756979 0.03824711 0.03887271 0.03944826\n",
      " 0.03997564]\n",
      "Prediction: [0.03717084 0.03802128 0.03876381 0.03945621 0.04010933 0.04114649\n",
      " 0.04187158]\n",
      "================================================\n",
      "[[0.00608224]\n",
      " [0.00608224]\n",
      " [0.00608224]\n",
      " [0.00608224]\n",
      " [0.00608224]\n",
      " [0.00608224]\n",
      " [0.00608224]\n",
      " [0.00608225]\n",
      " [0.00608225]\n",
      " [0.00608225]\n",
      " [0.00608225]\n",
      " [0.00608225]\n",
      " [0.00608225]\n",
      " [0.00608225]\n",
      " [0.00608225]\n",
      " [0.00608225]\n",
      " [0.00608225]\n",
      " [0.00608225]\n",
      " [0.00608225]\n",
      " [0.00608225]]\n",
      "Expected removed rate: [0.00608225 0.00608225 0.00608225 0.00608225 0.00608225 0.00608225\n",
      " 0.00608225]\n",
      "Prediction: [0.00576492 0.0058695  0.00603515 0.00615078 0.00632564 0.00632485\n",
      " 0.00645277]\n",
      "================================================\n",
      "[[0.00011613]\n",
      " [0.00014583]\n",
      " [0.00018135]\n",
      " [0.0002237 ]\n",
      " [0.00027404]\n",
      " [0.00033369]\n",
      " [0.00040415]\n",
      " [0.00048716]\n",
      " [0.00058466]\n",
      " [0.00069886]\n",
      " [0.00083225]\n",
      " [0.00098761]\n",
      " [0.00116154]\n",
      " [0.00135571]\n",
      " [0.00157189]\n",
      " [0.00181192]\n",
      " [0.0020777 ]\n",
      " [0.0023712 ]\n",
      " [0.00269441]\n",
      " [0.00304939]]\n",
      "Expected removed rate: [0.00343817 0.00386282 0.00431835 0.00480567 0.00532553 0.00587853\n",
      " 0.00646514]\n",
      "Prediction: [0.00334655 0.00363037 0.00390297 0.00410278 0.004463   0.00474128\n",
      " 0.00503261]\n",
      "================================================\n",
      "[[0.0444332 ]\n",
      " [0.04443323]\n",
      " [0.04443326]\n",
      " [0.04443327]\n",
      " [0.04443328]\n",
      " [0.04443329]\n",
      " [0.0444333 ]\n",
      " [0.0444333 ]\n",
      " [0.0444333 ]\n",
      " [0.0444333 ]\n",
      " [0.0444333 ]\n",
      " [0.0444333 ]\n",
      " [0.0444333 ]\n",
      " [0.0444333 ]\n",
      " [0.0444333 ]\n",
      " [0.0444333 ]\n",
      " [0.0444333 ]\n",
      " [0.0444333 ]\n",
      " [0.0444333 ]\n",
      " [0.0444333 ]]\n",
      "Expected removed rate: [0.0444333 0.0444333 0.0444333 0.0444333 0.0444333 0.0444333 0.0444333]\n",
      "Prediction: [0.04390172 0.0440769  0.04423356 0.04443071 0.04474632 0.04462614\n",
      " 0.04478324]\n",
      "================================================\n",
      "[[0.01263832]\n",
      " [0.01266066]\n",
      " [0.01267934]\n",
      " [0.01269486]\n",
      " [0.0127077 ]\n",
      " [0.01271826]\n",
      " [0.01272691]\n",
      " [0.01273394]\n",
      " [0.01273964]\n",
      " [0.01274422]\n",
      " [0.01274789]\n",
      " [0.01275082]\n",
      " [0.01275311]\n",
      " [0.0127549 ]\n",
      " [0.01275629]\n",
      " [0.01275736]\n",
      " [0.01275818]\n",
      " [0.0127588 ]\n",
      " [0.01275928]\n",
      " [0.01275963]]\n",
      "Expected removed rate: [0.0127599  0.0127601  0.01276024 0.01276035 0.01276042 0.01276048\n",
      " 0.01276052]\n",
      "Prediction: [0.01236533 0.01248054 0.0126449  0.01277705 0.01296797 0.01294344\n",
      " 0.01307297]\n",
      "================================================\n",
      "[[1.0058647e-05]\n",
      " [1.4059471e-05]\n",
      " [1.9156436e-05]\n",
      " [2.5615780e-05]\n",
      " [3.3761688e-05]\n",
      " [4.3987584e-05]\n",
      " [5.6769386e-05]\n",
      " [7.2680967e-05]\n",
      " [9.2412149e-05]\n",
      " [1.1678961e-04]\n",
      " [1.4680096e-04]\n",
      " [1.8362245e-04]\n",
      " [2.2734549e-04]\n",
      " [2.7909715e-04]\n",
      " [3.4016062e-04]\n",
      " [4.1199217e-04]\n",
      " [4.9623888e-04]\n",
      " [5.9475761e-04]\n",
      " [7.0963457e-04]\n",
      " [8.4320561e-04]]\n",
      "Expected removed rate: [0.00099808 0.00117715 0.00138036 0.00161035 0.00186991 0.00216206\n",
      " 0.00248999]\n",
      "Prediction: [0.00071814 0.00086903 0.001066   0.00119436 0.00141745 0.00151442\n",
      " 0.00169049]\n",
      "================================================\n",
      "[[0.02482244]\n",
      " [0.02482277]\n",
      " [0.02482301]\n",
      " [0.02482319]\n",
      " [0.02482332]\n",
      " [0.02482341]\n",
      " [0.02482348]\n",
      " [0.02482352]\n",
      " [0.02482356]\n",
      " [0.02482358]\n",
      " [0.02482359]\n",
      " [0.0248236 ]\n",
      " [0.02482361]\n",
      " [0.02482362]\n",
      " [0.02482362]\n",
      " [0.02482362]\n",
      " [0.02482362]\n",
      " [0.02482362]\n",
      " [0.02482362]\n",
      " [0.02482363]]\n",
      "Expected removed rate: [0.02482363 0.02482363 0.02482363 0.02482363 0.02482363 0.02482363\n",
      " 0.02482363]\n",
      "Prediction: [0.0243576  0.02449554 0.02465549 0.02481334 0.0250493  0.02498563\n",
      " 0.02512397]\n",
      "================================================\n",
      "[[0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [1.2289153e-10]\n",
      " [7.2697465e-10]\n",
      " [2.3971909e-09]\n",
      " [6.0456387e-09]\n",
      " [1.3049483e-08]\n",
      " [2.5438689e-08]\n",
      " [4.6149804e-08]\n",
      " [7.9366892e-08]\n",
      " [1.3097693e-07]\n",
      " [2.0917501e-07]\n",
      " [3.2147676e-07]\n",
      " [4.8004432e-07]\n",
      " [7.0075350e-07]\n",
      " [1.0042038e-06]\n",
      " [1.4169788e-06]\n",
      " [1.9732111e-06]\n",
      " [2.7165231e-06]\n",
      " [3.7024201e-06]]\n",
      "Expected removed rate: [5.0012382e-06 6.7017586e-06 8.8208299e-06 1.1447593e-05 1.4687757e-05\n",
      " 1.8666275e-05 2.3530365e-05]\n",
      "Prediction: [-2.4527311e-05 -1.5087054e-05  1.7009676e-05  1.1790544e-04\n",
      "  2.7668849e-04  2.9830262e-04  4.2463839e-04]\n",
      "================================================\n",
      "[[0.00346927]\n",
      " [0.00384559]\n",
      " [0.0042455 ]\n",
      " [0.00466931]\n",
      " [0.0051172 ]\n",
      " [0.00558919]\n",
      " [0.00608518]\n",
      " [0.0066049 ]\n",
      " [0.00714791]\n",
      " [0.00771362]\n",
      " [0.00830126]\n",
      " [0.00890987]\n",
      " [0.00953344]\n",
      " [0.01017041]\n",
      " [0.01081908]\n",
      " [0.01147764]\n",
      " [0.01214417]\n",
      " [0.01281665]\n",
      " [0.01349299]\n",
      " [0.01417102]]\n",
      "Expected removed rate: [0.01484856 0.01552338 0.01618674 0.01683665 0.01747119 0.01808861\n",
      " 0.01868726]\n",
      "Prediction: [0.01556297 0.01616285 0.01666532 0.01709506 0.01767984 0.01839464\n",
      " 0.01894195]\n",
      "================================================\n",
      "[[0.01160838]\n",
      " [0.01169298]\n",
      " [0.01176777]\n",
      " [0.01183361]\n",
      " [0.01189131]\n",
      " [0.01194164]\n",
      " [0.01198534]\n",
      " [0.01202311]\n",
      " [0.01205559]\n",
      " [0.0120834 ]\n",
      " [0.0121071 ]\n",
      " [0.01212718]\n",
      " [0.01214404]\n",
      " [0.01215814]\n",
      " [0.01216985]\n",
      " [0.01217953]\n",
      " [0.0121875 ]\n",
      " [0.01219402]\n",
      " [0.01219932]\n",
      " [0.01220361]]\n",
      "Expected removed rate: [0.01220706 0.01220983 0.01221201 0.01221373 0.01221508 0.01221612\n",
      " 0.01221693]\n",
      "Prediction: [0.01179639 0.011912   0.01208331 0.01222027 0.01239978 0.0123835\n",
      " 0.01251275]\n",
      "================================================\n",
      "[[0.00749742]\n",
      " [0.00872982]\n",
      " [0.01008697]\n",
      " [0.01157706]\n",
      " [0.01320823]\n",
      " [0.01498845]\n",
      " [0.01692541]\n",
      " [0.01902639]\n",
      " [0.02129817]\n",
      " [0.02374688]\n",
      " [0.02637783]\n",
      " [0.02919542]\n",
      " [0.03211099]\n",
      " [0.03511836]\n",
      " [0.03821046]\n",
      " [0.0413794 ]\n",
      " [0.04461647]\n",
      " [0.04791224]\n",
      " [0.05125659]\n",
      " [0.05463884]]\n",
      "Expected removed rate: [0.05804781 0.06147193 0.06484337 0.06815145 0.07138599 0.07453746\n",
      " 0.07759701]\n",
      "Prediction: [0.06225879 0.06473383 0.06646411 0.06805652 0.07031661 0.07362615\n",
      " 0.07577147]\n",
      "================================================\n",
      "[[0.13234763]\n",
      " [0.13572556]\n",
      " [0.13887087]\n",
      " [0.14178817]\n",
      " [0.14448334]\n",
      " [0.14696336]\n",
      " [0.14923619]\n",
      " [0.15131064]\n",
      " [0.15319622]\n",
      " [0.15490292]\n",
      " [0.1564412 ]\n",
      " [0.15782171]\n",
      " [0.15904611]\n",
      " [0.16012724]\n",
      " [0.16107759]\n",
      " [0.16190918]\n",
      " [0.16263352]\n",
      " [0.16326149]\n",
      " [0.16380334]\n",
      " [0.16426866]]\n",
      "Expected removed rate: [0.16466631 0.16500446 0.16528842 0.16552567 0.16572286 0.16588593\n",
      " 0.16602002]\n",
      "Prediction: [0.16261706 0.16317888 0.16390225 0.1647017  0.16511959 0.16525623\n",
      " 0.16567153]\n",
      "================================================\n",
      "[[6.1434239e-06]\n",
      " [8.2251327e-06]\n",
      " [1.0874997e-05]\n",
      " [1.4230400e-05]\n",
      " [1.8458468e-05]\n",
      " [2.3761857e-05]\n",
      " [3.0385501e-05]\n",
      " [3.8624516e-05]\n",
      " [4.8833368e-05]\n",
      " [6.1436527e-05]\n",
      " [7.6940749e-05]\n",
      " [9.5949210e-05]\n",
      " [1.1814907e-04]\n",
      " [1.4399305e-04]\n",
      " [1.7398555e-04]\n",
      " [2.0868672e-04]\n",
      " [2.4871647e-04]\n",
      " [2.9475841e-04]\n",
      " [3.4756382e-04]\n",
      " [4.0795523e-04]]\n",
      "Expected removed rate: [0.00047683 0.00055516 0.00064145 0.00073623 0.00084007 0.00095351\n",
      " 0.00107712]\n",
      "Prediction: [0.00022557 0.00034718 0.00052927 0.00064357 0.00083292 0.00089097\n",
      " 0.00104121]\n",
      "================================================\n",
      "[[0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [1.3045916e-10]\n",
      " [8.1123219e-10]\n",
      " [2.8094023e-09]\n",
      " [7.4430599e-09]\n",
      " [1.6886084e-08]\n",
      " [3.4619003e-08]\n",
      " [6.6089726e-08]\n",
      " [1.1967383e-07]\n",
      " [2.0806027e-07]\n",
      " [3.5023763e-07]\n",
      " [5.6262894e-07]\n",
      " [8.7457931e-07]\n",
      " [1.3262458e-06]\n",
      " [1.9722227e-06]\n",
      " [2.8862951e-06]\n",
      " [4.1676440e-06]\n",
      " [5.9489230e-06]\n",
      " [8.4067324e-06]]\n",
      "Expected removed rate: [1.17751515e-05 1.63631776e-05 2.22900271e-05 2.99062358e-05\n",
      " 3.96455980e-05 5.20431131e-05 6.77564894e-05]\n",
      "Prediction: [-2.4102628e-05 -1.4632568e-05  2.1681190e-05  1.2269616e-04\n",
      "  2.8186664e-04  3.0385703e-04  4.3045357e-04]\n",
      "================================================\n",
      "[[0.00676177]\n",
      " [0.00706331]\n",
      " [0.00735651]\n",
      " [0.00764065]\n",
      " [0.00791506]\n",
      " [0.00817915]\n",
      " [0.00843242]\n",
      " [0.00867445]\n",
      " [0.00890491]\n",
      " [0.00912356]\n",
      " [0.00933023]\n",
      " [0.00952485]\n",
      " [0.00970615]\n",
      " [0.00987437]\n",
      " [0.01002987]\n",
      " [0.01017302]\n",
      " [0.01030429]\n",
      " [0.01042418]\n",
      " [0.01053322]\n",
      " [0.01063197]]\n",
      "Expected removed rate: [0.01072104 0.01080104 0.01087185 0.01093427 0.01098903 0.01103686\n",
      " 0.01107844]\n",
      "Prediction: [0.01055864 0.01075889 0.01101376 0.01121982 0.01142424 0.0115644\n",
      " 0.01176405]\n"
     ]
    }
   ],
   "source": [
    "display_limit = 30\n",
    "for inputs, pred_removed, exp_removed in zip(X_temporal_test, predictions[:display_limit], Y_removed_test[:display_limit]):\n",
    "    print(\"================================================\")\n",
    "    print(inputs)\n",
    "    print(\"Expected removed rate:\", exp_removed)\n",
    "    print(\"Prediction:\", pred_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
